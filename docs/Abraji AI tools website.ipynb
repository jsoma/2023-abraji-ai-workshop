{
 "cells": [
  {
   "cell_type": "raw",
   "id": "b7d4e32f",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"AI Tools for Journalism\"\n",
    "project:\n",
    "  type: website\n",
    "  output-dir: docs\n",
    "format:\n",
    "  html:\n",
    "    toc: true\n",
    "    page-layout: full\n",
    "    embed-resources: false\n",
    "    theme:\n",
    "        - cosmo\n",
    "        - custom.css\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d38dd8a",
   "metadata": {},
   "source": [
    "Abraji Conference 2023\n",
    "\n",
    "Jonathan Soma\n",
    "\n",
    "[js4571@columbia.edu](mailto:js4571@columbia.edu)\n",
    "\n",
    "[@dangerscarf](https://twitter.com/dangerscarf/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80d148d",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Models are what AI and machine learning are both based on. You have probably heard of some:\n",
    "\n",
    "* **Text models:** GPT-2, GPT-3, GPT-3.5-turbo, GPT-4, Bard, Claude, LLaMa\n",
    "* **Image models:** Stable Diffusion, Midjourney, DALL-E 2, Imagen \n",
    "\n",
    "You give them an input, they give you an output. It sounds like a silly definition, but it's true!\n",
    "\n",
    "![How models work](images/how-models-work.png)\n",
    "\n",
    "There are several major categories of models.\n",
    "\n",
    "* Models that are trained on small amounts of data, just for one job\n",
    "* Models that are trained on large amounts of data, then fine-tuned for one job\n",
    "* Models that are trained on HUGE amounts of data and can already do everything\n",
    "\n",
    "## Classification\n",
    "\n",
    "### Sentiment analysis\n",
    "\n",
    "Sentiment analysis says whether something is **positive** or **negative**. If we wanted to change the image above to reflect the specific knowledge of this model, it might look like this:\n",
    "\n",
    "![A sentiment analysis model](images/sa-model.png)\n",
    "\n",
    "**Let's try sentiment analysis!** Here is a small example, using the `distilbert-base-uncased-finetuned-sst-2-english`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec753d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we'll install the \"transformers\" library from Hugging Face\n",
    "!pip install -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00339d6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a3ab1344f834f49bb218ac900fa6349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2f9cc0d55b445e1aa190c00e7656207",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/255M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f20506ea32d64ad2ad886545276d2e56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bbedb03b4694d648741a8107e9875a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9996883869171143}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\",\n",
    "                             model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "data = [\"I enjoyed eating the sandwich\"]\n",
    "\n",
    "sentiment_pipeline(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d562156",
   "metadata": {},
   "source": [
    "Models are not perfect, and they only know what they know. If we [read the details of the model](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) we learn it is based on English. Because of this, maybe it is not very good at Portuguese?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9efd042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.5782410502433777}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_pipeline = pipeline(\"sentiment-analysis\",\n",
    "                             model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "data = [\"Gostei de comer o sanduíche\"]\n",
    "\n",
    "sentiment_pipeline(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab20fad1",
   "metadata": {},
   "source": [
    "If we want to use Portuguese, we probably need a multi-lingual model. We could also find a model that *only* knows Portuguese, but it's probably easier to find a model that speaks many languages and includes Portuguese.\n",
    "\n",
    "We use [the Hugging Face website](https://huggingface.co/models?language=multilingual&sort=downloads&search=sentiment) to find one that looks promising:\n",
    "\n",
    "![Searching the Hugging Face model hub](images/searching-for-pt.png)\n",
    "\n",
    "We can read [details here](https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment), which includes this paragraph:\n",
    "\n",
    "> This is a multilingual XLM-roBERTa-base model trained on ~198M tweets and finetuned for sentiment analysis. The sentiment fine-tuning was done on 8 languages (Ar, En, Fr, De, Hi, It, Sp, Pt) but it can be used for more languages (see paper for details).\n",
    "\n",
    "`Pt` is Portuguese, so it sounds good to me!\n",
    "\n",
    "To use this new model, which we just replace `model=\"...\"` with its name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a420227f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66e053eb159f4dd3a80e808e61a9d190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/841 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2859d3bcda58485dae5795d6876a0189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.04G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c5e3348ffbe49fa845155663dc41add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading sentencepiece.bpe.model:   0%|          | 0.00/4.83M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ebd2cb5f4e44a2195fa8a71383308f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'positive', 'score': 0.8415056467056274}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_pipeline = pipeline(\"sentiment-analysis\",\n",
    "                             model=\"cardiffnlp/twitter-xlm-roberta-base-sentiment\")\n",
    "data = [\"Gostei de comer o sanduíche\"]\n",
    "\n",
    "sentiment_pipeline(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cc3ae9",
   "metadata": {},
   "source": [
    "Wonderful! So far we have learned:\n",
    "    \n",
    "* There isn’t just one “sentiment analysis” – different models give different results!\n",
    "* Different models might perform better or worse at different tasks or languages\n",
    "\n",
    "This is because they are **trained** or **fine-tuned** in different ways.\n",
    "\n",
    "### Training custom models\n",
    "\n",
    "For sentiment analysis, one traditional way of training a model would be to show it examples of positive or negative sentences, and say \"this one is positive!\" or \"this one is negative!\" Over time the machine learns which words are associated with a positive sentence, and which are associated with a negative sentence.\n",
    "\n",
    "![Training a model](images/training-1.png)\n",
    "\n",
    "This doesn't have to be about sentiment analysis, though! You can use this to put text into *any* type of categoy. A **classification model** is the general name for a model that puts content into different categories (or classes).\n",
    "\n",
    "![Classification model](images/classification-model.png)\n",
    "\n",
    "Classification models are common in **investigative journalism** when you have to analyze large amounts of data. You read several hundred documents, and mark the ones you are interested in. You use this to train a model, and the model then reads the rest for you, marking the interesting ones.\n",
    "\n",
    "For example, the Washington Post had a model [read thousands of app store reviews](https://www.washingtonpost.com/technology/2019/11/22/apple-says-its-app-store-is-safe-trusted-place-we-found-reports-unwanted-sexual-behavior-six-apps-some-targeting-minors/) to find ones about unwanted sexual behavior on random chat apps. You can see a technical writeup I made of it [here](https://investigate.ai/wapo-app-reviews/predict-reviews/).\n",
    "\n",
    "Another example, the LA Times used a classifier to [examine crimes that were classified as either minor or serious](https://www.latimes.com/local/la-me-crimestats-lapd-20140810-story.html), and determined the Los Angeles Police Department was misclassifying crimes. You can find my walkthroughs of that [here](https://investigate.ai/latimes-crime-classification/using-a-classifier-to-find-misclassified-crimes/) or [here](https://investigate.ai/latimes-crime-classification/walkthrough/).\n",
    "\n",
    "One problem with these models is it is difficult for them to learn words as easily as people. For example, the words below all relate to the same action, but a classifier might see them all as very different words.\n",
    "\n",
    "* Fish, fishes, fishing, fished\n",
    "* correr, correndo, correu, correm\n",
    "\n",
    "You can use [stemming and lemmatization](https://investigate.ai/text-analysis/stemming-and-lemmatization/) to fix the problem, but there are easier ways!\n",
    "\n",
    "### Fine-tuned models\n",
    "\n",
    "Instead of having a model learn language from zero, you can also use a pre-built model, then provide it a little extra information for your purpose. This is called \"fine tuning.\"\n",
    "\n",
    "For example, we can look at [the details](https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment) for the `cardiffnlp/twitter-xlm-roberta-base-sentiment` model we used before:\n",
    "\n",
    "> This is a multilingual XLM-roBERTa-base model **trained on ~198M tweets** and **finetuned for sentiment analysis**. The sentiment fine-tuning was done on 8 languages (Ar, En, Fr, De, Hi, It, Sp, Pt) but it can be used for more languages (see paper for details).\n",
    "\n",
    "It read almost 200 million tweets to learn how the language in tweets works, then it was taught positive and negative sentiment afterwards. These models almost always perform better than the small custom models from the previous section.\n",
    "\n",
    "To make a fine-tuned model, you can use the [Hugging Face AutoTrainer](https://ui.autotrain.huggingface.co/) to make it very easily. You upload a spreadsheet of your marked dataset, then it fine-tunes several models for you, and shows you the one that performed the best.\n",
    "\n",
    "For example, [this is a fine-tuned model](https://huggingface.co/wendys-llc/creepy-wapo) I made for the Washington Post comments project. You can use it exactly like the sentiment analysis one! Let's try it with two sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4551752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1c3562d600a4a6baed2c6e488302e3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.33k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'label': '0.0', 'score': 0.998849630355835},\n",
       " {'label': '1.0', 'score': 0.8436580300331116}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_pipeline = pipeline(model=\"wendys-llc/creepy-wapo\")\n",
    "data = [\n",
    "    \"I love the app, talking to people is fun\",\n",
    "    \"Be careful talking to men, they all want nudes :(\"\n",
    "]\n",
    "\n",
    "sentiment_pipeline(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b44a327",
   "metadata": {},
   "source": [
    "You can see the first one is marked `0` (not creepy) and the second is marked `1` (creepy behavior). They also get a score as to how confident the model is that it's making an accurate decision.\n",
    "\n",
    "This model is based on a fine-tuned version of [DeBERTa](https://huggingface.co/docs/transformers/model_doc/deberta-v2), a language model from Microsoft. You don't have to pick the model: Hugging Face automatically fine-tunes five different language models for you and you can pick the best-performing one!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6945be51",
   "metadata": {},
   "source": [
    "### Large language models\n",
    "\n",
    "Large language models have changed everything! They are \"large\" because they know a lot more than previous models, and can do many tasks *without* fine tuning. They've read so much of the internet that they automatically know what positive and negative statements are!\n",
    "\n",
    "You might be familiar with using ChatGPT to talk to the GPT model, but you can also use it in Python through an API. You need to [register for an API key](https://platform.openai.com/) for the code below to work.\n",
    "\n",
    "I personally like to use tools like GPT with [Langchain](https://python.langchain.com/docs/get_started/quickstart), a library that makes working with large language models a little easier. After you set up an API key, you then need to install the libraries for both `openai` and `langchain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b194153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "#| output: false\n",
    "!pip install -q --upgrade openai langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fd944c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here's a classic recipe for chocolate chip cookies:\n",
      "\n",
      "Ingredients:\n",
      "- 1 cup unsalted butter, softened\n",
      "- 1 cup granulated sugar\n",
      "- 1 cup packed brown sugar\n",
      "- 2 large eggs\n",
      "- 1 teaspoon vanilla extract\n",
      "- 3 cups all-purpose flour\n",
      "- 1 teaspoon baking soda\n",
      "- 1/2 teaspoon salt\n",
      "- 2 cups chocolate chips\n",
      "\n",
      "Instructions:\n",
      "1. Preheat your oven to 375°F (190°C). Line a baking sheet with parchment paper or silicone baking mat.\n",
      "\n",
      "2. In a large mixing bowl, cream together the softened butter, granulated sugar, and brown sugar until light and fluffy.\n",
      "\n",
      "3. Beat in the eggs, one at a time, followed by the vanilla extract. Mix well after each addition.\n",
      "\n",
      "4. In a separate bowl, whisk together the flour, baking soda, and salt. Gradually add the dry ingredients to the wet ingredients and mix until just combined. Avoid overmixing.\n",
      "\n",
      "5. Stir in the chocolate chips until evenly distributed throughout the dough.\n",
      "\n",
      "6. Using a cookie scoop or a tablespoon, drop rounded balls of dough onto the prepared baking sheet, spacing them about 2 inches apart.\n",
      "\n",
      "7. Bake in the preheated oven for 9-11 minutes, or until the cookies are golden brown around the edges. The centers may still appear slightly undercooked, but they will firm up as they cool.\n",
      "\n",
      "8. Allow the cookies to cool on the baking sheet for 5 minutes, then transfer them to a wire rack to cool completely.\n",
      "\n",
      "9. Enjoy your homemade chocolate chip cookies with a glass of milk or as a sweet treat!\n",
      "\n",
      "Note: Feel free to customize your cookies by adding chopped nuts, using different types of chocolate chips, or incorporating other mix-ins like shredded coconut or dried fruit.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "API_KEY = \"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\"\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=API_KEY, model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "response = llm.predict(\"Give me a recipe for chocolate-chip cookies\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d9500a",
   "metadata": {},
   "source": [
    "### Zero-shot classification\n",
    "\n",
    "We can now create a classifier using GPT *without training it at all*. We just need to tell it what we are looking for! This is called **zero-shot classification** because it takes zero examples for the classifier to learn.\n",
    "\n",
    "For example, maybe I want to classify the text of a bill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43f0132f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENVIRONMENT\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Categorize the following text as being about ENVIRONMENT, GUN CONTROL,\n",
    "or IMMIGRATION. Respond with only the category.\n",
    "\n",
    "Text: A Bill to Regulate the Sulfur Emissions of Coal-Fired Energy\n",
    "Plants in the State of New York.\n",
    "\"\"\"\n",
    "\n",
    "response = llm.predict(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6716628",
   "metadata": {},
   "source": [
    "It's so easy!!! You can also make use of [Python's `.format` method](https://www.w3schools.com/python/ref_string_format.asp) to make things even simpler if you want to make a lot of requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9bb5c029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Categorize the following text as being about ENVIRONMENT, GUN CONTROL,\n",
      "or IMMIGRATION. Respond with only the category.\n",
      "\n",
      "Text: This fills in the spot in the template\n",
      "\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "Categorize the following text as being about ENVIRONMENT, GUN CONTROL,\n",
    "or IMMIGRATION. Respond with only the category.\n",
    "\n",
    "Text: {bill_text}\n",
    "\"\"\"\n",
    "\n",
    "print(template.format(bill_text=\"This fills in the spot in the template\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e7f1272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Bill to Allow Additional Refugees In Upstate New York is IMMIGRATION\n",
      "A Bill to Close Down Coal-fired Power Plants is ENVIRONMENT\n",
      "A Bill to Banning Assault Rifles at Public Events is GUN CONTROL\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "Categorize the following text as being about ENVIRONMENT, GUN CONTROL,\n",
    "or IMMIGRATION. Respond with only the category.\n",
    "\n",
    "Text: {bill_text}\n",
    "\"\"\"\n",
    "\n",
    "bills = [\n",
    "    \"A Bill to Allow Additional Refugees In Upstate New York\",\n",
    "    \"A Bill to Close Down Coal-fired Power Plants\",\n",
    "    \"A Bill to Banning Assault Rifles at Public Events\"\n",
    "]\n",
    "\n",
    "for bill in bills:\n",
    "    prompt = template.format(bill_text=bill)\n",
    "    response = llm.predict(prompt)\n",
    "    print(bill, \"is\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a130ec1",
   "metadata": {},
   "source": [
    "There is also **few-shot classification** which provides several examples to the LLM when you want to classify something. For example, I might provide two or three tricky situations or edge cases in the prompt so the classifier knows what I really want."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4994b977",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "\n",
    "Named entity recognition is the ability to recognize people, places, companies, and other things inside of your text. It's like classification, but for *parts* of a text. It's often used to extract the names of people from documents.\n",
    "\n",
    "### Using spaCy\n",
    "\n",
    "Traditionally, you would use a tool like [spaCy](https://spacy.io/) to do this. You'd probably still do that, honestly, it works pretty well and (unlike GPT models) it's free!\n",
    "\n",
    "We'll start by installing spaCy and downloading two models: one is a medium-sized English model, and the other is focused on Portuguese. You can find these models on [the spaCy usage page](https://spacy.io/usage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e2278f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting en-core-web-md==3.4.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.4.1/en_core_web_md-3.4.1-py3-none-any.whl (42.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /Users/soma/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from en-core-web-md==3.4.1) (3.4.1)\n",
      "Requirement already satisfied: setuptools in /Users/soma/.pyenv/versions/3.10.3/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (58.1.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/soma/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.4.4)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/soma/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.0.8)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/soma/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.23.5)\n",
      "Requirement already satisfied: jinja2 in /Users/soma/.pyenv/versions/3.10.3/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (3.1.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/soma/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.0.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/soma/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.0.6)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/soma/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.4.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /Users/soma/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.9.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/soma/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.0.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/soma/.pyenv/versions/3.10.3/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.31.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /Users/soma/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (8.1.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/soma/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.6.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/soma/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (3.0.6)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/soma/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (3.3.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /Users/soma/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (3.0.9)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /Users/soma/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.10.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/soma/.pyenv/versions/3.10.3/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (4.65.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/soma/.pyenv/versions/3.10.3/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/soma/.pyenv/versions/3.10.3/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /Users/soma/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/soma/.pyenv/versions/3.10.3/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (3.10.0.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/soma/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/soma/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.0.12)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/soma/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/soma/.pyenv/versions/3.10.3/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2022.12.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/soma/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.7.8)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/soma/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/soma/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.1.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n",
      "Collecting pt-core-news-lg==3.4.0\n",
      "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='objects.githubusercontent.com', port=443): Read timed out. (read timeout=15)\")': /github-production-release-asset-2e65be/84940268/dbd09dd2-2f19-4ddc-844b-d6350555cbc5?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230701%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230701T145127Z&X-Amz-Expires=300&X-Amz-Signature=1c190563eb29c5d3806cabe27a279414a9420c2387d1132c1a9b869947a8c72a&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=84940268&response-content-disposition=attachment%3B%20filename%3Dpt_core_news_lg-3.4.0-py3-none-any.whl&response-content-type=application%2Foctet-stream\u001b[0m\u001b[33m\n",
      "\u001b[0m  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_lg-3.4.0/pt_core_news_lg-3.4.0-py3-none-any.whl (568.2 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m568.2/568.2 MB\u001b[0m \u001b[31m694.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:05\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /Users/soma/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from pt-core-news-lg==3.4.0) (3.4.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pathy>=0.3.5 in /Users/soma/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-lg==3.4.0) (0.6.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/soma/.pyenv/versions/3.10.3/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-lg==3.4.0) (2.31.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/soma/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-lg==3.4.0) (3.3.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/soma/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-lg==3.4.0) (1.0.7)\n",
      "Requirement already satisfied: jinja2 in /Users/soma/.pyenv/versions/3.10.3/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-lg==3.4.0) (3.1.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/soma/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-lg==3.4.0) (2.0.8)\n",
      "Requirement already satisfied: setuptools in /Users/soma/.pyenv/versions/3.10.3/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-lg==3.4.0) (58.1.0)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/soma/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-lg==3.4.0) (0.4.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/soma/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-lg==3.4.0) (1.23.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /Users/soma/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-lg==3.4.0) (1.9.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /Users/soma/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-lg==3.4.0) (8.1.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/soma/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-lg==3.4.0) (2.0.6)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/soma/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-lg==3.4.0) (1.0.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /Users/soma/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-lg==3.4.0) (3.0.9)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/soma/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-lg==3.4.0) (3.0.6)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/soma/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-lg==3.4.0) (2.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/soma/.pyenv/versions/3.10.3/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-lg==3.4.0) (21.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /Users/soma/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-lg==3.4.0) (0.10.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/soma/.pyenv/versions/3.10.3/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-lg==3.4.0) (4.65.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/soma/.pyenv/versions/3.10.3/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->pt-core-news-lg==3.4.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /Users/soma/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->pt-core-news-lg==3.4.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/soma/.pyenv/versions/3.10.3/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy<3.5.0,>=3.4.0->pt-core-news-lg==3.4.0) (3.10.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/soma/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->pt-core-news-lg==3.4.0) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/soma/.pyenv/versions/3.10.3/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->pt-core-news-lg==3.4.0) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/soma/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->pt-core-news-lg==3.4.0) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/soma/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->pt-core-news-lg==3.4.0) (1.26.11)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/soma/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->pt-core-news-lg==3.4.0) (0.7.8)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/soma/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->pt-core-news-lg==3.4.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/soma/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->pt-core-news-lg==3.4.0) (2.1.2)\n",
      "Installing collected packages: pt-core-news-lg\n",
      "Successfully installed pt-core-news-lg-3.4.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('pt_core_news_lg')\n"
     ]
    }
   ],
   "source": [
    "#| output: false\n",
    "\n",
    "!pip install -q spacy\n",
    "!python -m spacy download en_core_web_md\n",
    "!python -m spacy download pt_core_news_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e8304a",
   "metadata": {},
   "source": [
    "Now let's try them both out. We'll start with `en_core_web_md` for English text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c2595f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jonathan Soma PERSON\n",
      "the Abraji Conference FAC\n",
      "Sāo Paulo GPE\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "text = \"Hi, I'm Jonathan Soma, and I'm giving this talk at the Abraji Conference in Sāo Paulo.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926f8379",
   "metadata": {},
   "source": [
    "Does it work as well for Portuguese?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5dca5e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jonathan Soma PERSON\n",
      "Congresso ORG\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "text = \"Olá, eu sou Jonathan Soma e estou dando esta palestra na Congresso da Abraji em São Paulo\"\n",
    "doc = nlp(text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a5b758",
   "metadata": {},
   "source": [
    "Maybe we can also try the specific Portuguese model to see if it performs any better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "605f44e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jonathan Soma PER\n",
      "Congresso da Abraji MISC\n",
      "São Paulo LOC\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_lg\")\n",
    "text = \"Olá, eu sou Jonathan Soma e estou dando esta palestra na Congresso da Abraji em São Paulo\"\n",
    "doc = nlp(text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fd851f",
   "metadata": {},
   "source": [
    "### Using an LLM\n",
    "\n",
    "In the same way we can use an LLM for classification, we can also use it for named entity recognition!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b35185d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jonathan Soma (PERSON), Abraji Conference (EVENT), Sāo Paulo (LOCATION)\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "API_KEY = \"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\"\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=API_KEY, model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "prompt = \"\"\"\n",
    "List the named entities in the text below. Use a comma to\n",
    "separate the entity and the type of entity. Valid entity\n",
    "types are PERSON, EVENT, ORGANIZATION, and LOCATION.\n",
    "\n",
    "Text: Hi, I'm Jonathan Soma, and I'm giving this talk at the Abraji Conference in Sāo Paulo.\n",
    "\"\"\"\n",
    "\n",
    "response = llm.predict(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdd7a88",
   "metadata": {},
   "source": [
    "I didn't really want those parentheses, but it's pretty good! Let's see how it works with Portuguese:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "81519ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jonathan Soma - PERSON\n",
      "Congresso da Abraji - EVENT\n",
      "São Paulo - LOCATION\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "List the named entities in the text below. Use a comma to\n",
    "separate the entity and the type of entity. Valid entity\n",
    "types are PERSON, EVENT, ORGANIZATION, and LOCATION.\n",
    "\n",
    "Text: Olá, eu sou Jonathan Soma e estou dando esta palestra na Congresso da Abraji em São Paulo.\n",
    "\"\"\"\n",
    "\n",
    "response = llm.predict(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d8a628",
   "metadata": {},
   "source": [
    "Notice it **didn't listen to my instructions perfectly!** Instead of using a comma to separate the entity and type, it used a hyphen instead. Large language models can be slightly unpredictabe, and you often need to be aggressive to keep it doing what you want."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc7c31a",
   "metadata": {},
   "source": [
    "## Document search and similarity\n",
    "\n",
    "Investigations often involve combing through documents that are in different languages. [How Quartz used AI to sort through the Luanda Leaks](https://qz.com/1786896/ai-for-investigations-sorting-through-the-luanda-leaks) gives a good background on the difficulty involved in the process!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "393c8924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "#| output: false\n",
    "!pip install -q sentence-transformers sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26feb8b2",
   "metadata": {},
   "source": [
    "In the code below we generate **text embeddings**. You can read [a writeup of mine about embeddings here](https://investigate.ai/text-analysis/word-embeddings/), but the short description is that words and sentences are turned into concepts. Instead of matches based on exact words, or even fish-fishes-fishing, you can match with general ideas.\n",
    "\n",
    "As a bonus: if you use a multi-lingual model, the same concepts can be matched even if the documents are in different languages!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "df05c12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0676569   0.06349581  0.0487131   0.07930496  0.03744796  0.00265277\n",
      "  0.03937485 -0.00709837  0.0593615   0.03153696  0.06009803 -0.05290522\n",
      "  0.04060676 -0.02593078  0.02984274  0.00112689  0.07351495 -0.05038185\n",
      " -0.12238666  0.02370274  0.02972649  0.04247681  0.0256338   0.00199517\n",
      " -0.05691912 -0.02715985 -0.03290359  0.06602488  0.11900704 -0.04587924\n",
      " -0.07262138 -0.03258408  0.05234135  0.04505523  0.00825302  0.03670237\n",
      " -0.01394151  0.06539196 -0.02642729  0.00020634 -0.01366437 -0.03628108\n",
      " -0.0195043  -0.02897387  0.03942709 -0.08840913  0.00262434  0.01367143\n",
      "  0.04830637 -0.03115652]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sentences = [\"This is an example sentence\"]\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings[0][:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480936c9",
   "metadata": {},
   "source": [
    "Those numbers above might look awful, but they are how computers think about concepts! If two pieces of text have similar numbers, they're probably about the same thing (...kind of).\n",
    "\n",
    "Let's look at a better example of how a single-language and multi-language embedding might make sentences more or less similar to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "336c783c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sentences = [\n",
    "    \"Molly ate a fish\",\n",
    "    \"Jen consumed a carp\",\n",
    "    \"I would like to sell you a house\",\n",
    "    \"Я пытаюсь купить дачу\", # I'm trying to buy a summer home\n",
    "    \"J'aimerais vous louer un grand appartement\", # I would like to rent a large apartment to you\n",
    "    \"This is a wonderful investment opportunity\",\n",
    "    \"Это прекрасная возможность для инвестиций\", # investment opportunity\n",
    "    \"C'est une merveilleuse opportunité d'investissement\", # investment opportunity\n",
    "    \"これは素晴らしい投資機会です\", # investment opportunity\n",
    "    \"野球はあなたが思うよりも面白いことがあります\", # baseball can be more interesting than you think\n",
    "    \"Baseball can be interesting than you'd think\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c3b3284f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c70a95f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_040a6_row0_col0, #T_040a6_row1_col1, #T_040a6_row2_col2, #T_040a6_row3_col3, #T_040a6_row4_col4, #T_040a6_row5_col5, #T_040a6_row6_col6, #T_040a6_row7_col7, #T_040a6_row8_col8, #T_040a6_row9_col9, #T_040a6_row10_col10 {\n",
       "  background-color: #023858;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_040a6_row0_col1, #T_040a6_row1_col0 {\n",
       "  background-color: #4e9ac6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_040a6_row0_col2, #T_040a6_row0_col8, #T_040a6_row2_col0, #T_040a6_row2_col9, #T_040a6_row8_col0, #T_040a6_row9_col2 {\n",
       "  background-color: #ebe6f2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_040a6_row0_col3, #T_040a6_row3_col0 {\n",
       "  background-color: #dddbec;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_040a6_row0_col4, #T_040a6_row0_col10, #T_040a6_row4_col0, #T_040a6_row5_col8, #T_040a6_row8_col5, #T_040a6_row10_col0 {\n",
       "  background-color: #ece7f2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_040a6_row0_col5, #T_040a6_row5_col0 {\n",
       "  background-color: #f8f1f8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_040a6_row0_col6, #T_040a6_row1_col3, #T_040a6_row3_col1, #T_040a6_row6_col0 {\n",
       "  background-color: #e9e5f1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_040a6_row0_col7, #T_040a6_row7_col0 {\n",
       "  background-color: #f7f0f7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_040a6_row0_col9, #T_040a6_row1_col10, #T_040a6_row2_col6, #T_040a6_row6_col2, #T_040a6_row9_col0, #T_040a6_row10_col1 {\n",
       "  background-color: #ede7f2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_040a6_row1_col2, #T_040a6_row2_col1 {\n",
       "  background-color: #e7e3f0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_040a6_row1_col4, #T_040a6_row4_col1 {\n",
       "  background-color: #f1ebf5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_040a6_row1_col5, #T_040a6_row2_col7, #T_040a6_row5_col1, #T_040a6_row5_col9, #T_040a6_row7_col2, #T_040a6_row9_col5 {\n",
       "  background-color: #f0eaf4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_040a6_row1_col6, #T_040a6_row6_col1 {\n",
       "  background-color: #faf2f8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_040a6_row1_col7, #T_040a6_row4_col10, #T_040a6_row7_col1, #T_040a6_row10_col4 {\n",
       "  background-color: #f1ebf4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_040a6_row1_col8, #T_040a6_row3_col7, #T_040a6_row7_col3, #T_040a6_row8_col1 {\n",
       "  background-color: #dedcec;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_040a6_row1_col9, #T_040a6_row9_col1 {\n",
       "  background-color: #d6d6e9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_040a6_row2_col3, #T_040a6_row3_col2 {\n",
       "  background-color: #d2d2e7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_040a6_row2_col4, #T_040a6_row4_col2 {\n",
       "  background-color: #e0dded;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_040a6_row2_col5, #T_040a6_row5_col2 {\n",
       "  background-color: #88b1d4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_040a6_row2_col8, #T_040a6_row8_col2 {\n",
       "  background-color: #eee8f3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_040a6_row2_col10, #T_040a6_row7_col10, #T_040a6_row10_col2, #T_040a6_row10_col7 {\n",
       "  background-color: #eee9f3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_040a6_row3_col4, #T_040a6_row4_col3 {\n",
       "  background-color: #d1d2e6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_040a6_row3_col5, #T_040a6_row5_col3 {\n",
       "  background-color: #e3e0ee;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_040a6_row3_col6, #T_040a6_row6_col3 {\n",
       "  background-color: #69a5cc;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_040a6_row3_col8, #T_040a6_row8_col3 {\n",
       "  background-color: #a1bbda;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_040a6_row3_col9, #T_040a6_row9_col3 {\n",
       "  background-color: #9cb9d9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_040a6_row3_col10, #T_040a6_row10_col3 {\n",
       "  background-color: #fff7fb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_040a6_row4_col5, #T_040a6_row5_col4 {\n",
       "  background-color: #eae6f1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_040a6_row4_col6, #T_040a6_row6_col4 {\n",
       "  background-color: #8fb4d6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_040a6_row4_col7, #T_040a6_row7_col4 {\n",
       "  background-color: #3d93c2;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_040a6_row4_col8, #T_040a6_row8_col4 {\n",
       "  background-color: #cdd0e5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_040a6_row4_col9, #T_040a6_row9_col4 {\n",
       "  background-color: #dad9ea;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_040a6_row5_col6, #T_040a6_row6_col5 {\n",
       "  background-color: #f3edf5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_040a6_row5_col7, #T_040a6_row7_col5 {\n",
       "  background-color: #c1cae2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_040a6_row5_col10, #T_040a6_row10_col5 {\n",
       "  background-color: #d9d8ea;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_040a6_row6_col7, #T_040a6_row7_col6 {\n",
       "  background-color: #acc0dd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_040a6_row6_col8, #T_040a6_row8_col6 {\n",
       "  background-color: #b0c2de;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_040a6_row6_col9, #T_040a6_row9_col6 {\n",
       "  background-color: #abbfdc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_040a6_row6_col10, #T_040a6_row10_col6 {\n",
       "  background-color: #f4eef6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_040a6_row7_col8, #T_040a6_row8_col7 {\n",
       "  background-color: #a8bedc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_040a6_row7_col9, #T_040a6_row9_col7 {\n",
       "  background-color: #c8cde4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_040a6_row8_col9, #T_040a6_row9_col8 {\n",
       "  background-color: #3790c0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_040a6_row8_col10, #T_040a6_row9_col10, #T_040a6_row10_col8, #T_040a6_row10_col9 {\n",
       "  background-color: #fdf5fa;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_040a6\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_040a6_level0_col0\" class=\"col_heading level0 col0\" >Molly ate a fish</th>\n",
       "      <th id=\"T_040a6_level0_col1\" class=\"col_heading level0 col1\" >Jen consumed a carp</th>\n",
       "      <th id=\"T_040a6_level0_col2\" class=\"col_heading level0 col2\" >I would like to sell you a house</th>\n",
       "      <th id=\"T_040a6_level0_col3\" class=\"col_heading level0 col3\" >Я пытаюсь купить дачу</th>\n",
       "      <th id=\"T_040a6_level0_col4\" class=\"col_heading level0 col4\" >J'aimerais vous louer un grand appartement</th>\n",
       "      <th id=\"T_040a6_level0_col5\" class=\"col_heading level0 col5\" >This is a wonderful investment opportunity</th>\n",
       "      <th id=\"T_040a6_level0_col6\" class=\"col_heading level0 col6\" >Это прекрасная возможность для инвестиций</th>\n",
       "      <th id=\"T_040a6_level0_col7\" class=\"col_heading level0 col7\" >C'est une merveilleuse opportunité d'investissement</th>\n",
       "      <th id=\"T_040a6_level0_col8\" class=\"col_heading level0 col8\" >これは素晴らしい投資機会です</th>\n",
       "      <th id=\"T_040a6_level0_col9\" class=\"col_heading level0 col9\" >野球はあなたが思うよりも面白いことがあります</th>\n",
       "      <th id=\"T_040a6_level0_col10\" class=\"col_heading level0 col10\" >Baseball can be interesting than you'd think</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_040a6_level0_row0\" class=\"row_heading level0 row0\" >Molly ate a fish</th>\n",
       "      <td id=\"T_040a6_row0_col0\" class=\"data row0 col0\" >1.000000</td>\n",
       "      <td id=\"T_040a6_row0_col1\" class=\"data row0 col1\" >0.526053</td>\n",
       "      <td id=\"T_040a6_row0_col2\" class=\"data row0 col2\" >0.025476</td>\n",
       "      <td id=\"T_040a6_row0_col3\" class=\"data row0 col3\" >0.098335</td>\n",
       "      <td id=\"T_040a6_row0_col4\" class=\"data row0 col4\" >0.020435</td>\n",
       "      <td id=\"T_040a6_row0_col5\" class=\"data row0 col5\" >-0.065293</td>\n",
       "      <td id=\"T_040a6_row0_col6\" class=\"data row0 col6\" >0.035801</td>\n",
       "      <td id=\"T_040a6_row0_col7\" class=\"data row0 col7\" >-0.062506</td>\n",
       "      <td id=\"T_040a6_row0_col8\" class=\"data row0 col8\" >0.027358</td>\n",
       "      <td id=\"T_040a6_row0_col9\" class=\"data row0 col9\" >0.017622</td>\n",
       "      <td id=\"T_040a6_row0_col10\" class=\"data row0 col10\" >0.023445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_040a6_level0_row1\" class=\"row_heading level0 row1\" >Jen consumed a carp</th>\n",
       "      <td id=\"T_040a6_row1_col0\" class=\"data row1 col0\" >0.526053</td>\n",
       "      <td id=\"T_040a6_row1_col1\" class=\"data row1 col1\" >1.000000</td>\n",
       "      <td id=\"T_040a6_row1_col2\" class=\"data row1 col2\" >0.044178</td>\n",
       "      <td id=\"T_040a6_row1_col3\" class=\"data row1 col3\" >0.035044</td>\n",
       "      <td id=\"T_040a6_row1_col4\" class=\"data row1 col4\" >-0.018194</td>\n",
       "      <td id=\"T_040a6_row1_col5\" class=\"data row1 col5\" >-0.004438</td>\n",
       "      <td id=\"T_040a6_row1_col6\" class=\"data row1 col6\" >-0.078566</td>\n",
       "      <td id=\"T_040a6_row1_col7\" class=\"data row1 col7\" >-0.011418</td>\n",
       "      <td id=\"T_040a6_row1_col8\" class=\"data row1 col8\" >0.090357</td>\n",
       "      <td id=\"T_040a6_row1_col9\" class=\"data row1 col9\" >0.131507</td>\n",
       "      <td id=\"T_040a6_row1_col10\" class=\"data row1 col10\" >0.016100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_040a6_level0_row2\" class=\"row_heading level0 row2\" >I would like to sell you a house</th>\n",
       "      <td id=\"T_040a6_row2_col0\" class=\"data row2 col0\" >0.025476</td>\n",
       "      <td id=\"T_040a6_row2_col1\" class=\"data row2 col1\" >0.044178</td>\n",
       "      <td id=\"T_040a6_row2_col2\" class=\"data row2 col2\" >1.000000</td>\n",
       "      <td id=\"T_040a6_row2_col3\" class=\"data row2 col3\" >0.154773</td>\n",
       "      <td id=\"T_040a6_row2_col4\" class=\"data row2 col4\" >0.083555</td>\n",
       "      <td id=\"T_040a6_row2_col5\" class=\"data row2 col5\" >0.386736</td>\n",
       "      <td id=\"T_040a6_row2_col6\" class=\"data row2 col6\" >0.017175</td>\n",
       "      <td id=\"T_040a6_row2_col7\" class=\"data row2 col7\" >-0.006744</td>\n",
       "      <td id=\"T_040a6_row2_col8\" class=\"data row2 col8\" >0.010857</td>\n",
       "      <td id=\"T_040a6_row2_col9\" class=\"data row2 col9\" >0.025510</td>\n",
       "      <td id=\"T_040a6_row2_col10\" class=\"data row2 col10\" >0.006353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_040a6_level0_row3\" class=\"row_heading level0 row3\" >Я пытаюсь купить дачу</th>\n",
       "      <td id=\"T_040a6_row3_col0\" class=\"data row3 col0\" >0.098335</td>\n",
       "      <td id=\"T_040a6_row3_col1\" class=\"data row3 col1\" >0.035044</td>\n",
       "      <td id=\"T_040a6_row3_col2\" class=\"data row3 col2\" >0.154773</td>\n",
       "      <td id=\"T_040a6_row3_col3\" class=\"data row3 col3\" >1.000000</td>\n",
       "      <td id=\"T_040a6_row3_col4\" class=\"data row3 col4\" >0.159519</td>\n",
       "      <td id=\"T_040a6_row3_col5\" class=\"data row3 col5\" >0.064379</td>\n",
       "      <td id=\"T_040a6_row3_col6\" class=\"data row3 col6\" >0.462397</td>\n",
       "      <td id=\"T_040a6_row3_col7\" class=\"data row3 col7\" >0.092110</td>\n",
       "      <td id=\"T_040a6_row3_col8\" class=\"data row3 col8\" >0.314708</td>\n",
       "      <td id=\"T_040a6_row3_col9\" class=\"data row3 col9\" >0.327675</td>\n",
       "      <td id=\"T_040a6_row3_col10\" class=\"data row3 col10\" >-0.119607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_040a6_level0_row4\" class=\"row_heading level0 row4\" >J'aimerais vous louer un grand appartement</th>\n",
       "      <td id=\"T_040a6_row4_col0\" class=\"data row4 col0\" >0.020435</td>\n",
       "      <td id=\"T_040a6_row4_col1\" class=\"data row4 col1\" >-0.018194</td>\n",
       "      <td id=\"T_040a6_row4_col2\" class=\"data row4 col2\" >0.083555</td>\n",
       "      <td id=\"T_040a6_row4_col3\" class=\"data row4 col3\" >0.159519</td>\n",
       "      <td id=\"T_040a6_row4_col4\" class=\"data row4 col4\" >1.000000</td>\n",
       "      <td id=\"T_040a6_row4_col5\" class=\"data row4 col5\" >0.032253</td>\n",
       "      <td id=\"T_040a6_row4_col6\" class=\"data row4 col6\" >0.365505</td>\n",
       "      <td id=\"T_040a6_row4_col7\" class=\"data row4 col7\" >0.566635</td>\n",
       "      <td id=\"T_040a6_row4_col8\" class=\"data row4 col8\" >0.172406</td>\n",
       "      <td id=\"T_040a6_row4_col9\" class=\"data row4 col9\" >0.110118</td>\n",
       "      <td id=\"T_040a6_row4_col10\" class=\"data row4 col10\" >-0.013743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_040a6_level0_row5\" class=\"row_heading level0 row5\" >This is a wonderful investment opportunity</th>\n",
       "      <td id=\"T_040a6_row5_col0\" class=\"data row5 col0\" >-0.065293</td>\n",
       "      <td id=\"T_040a6_row5_col1\" class=\"data row5 col1\" >-0.004438</td>\n",
       "      <td id=\"T_040a6_row5_col2\" class=\"data row5 col2\" >0.386736</td>\n",
       "      <td id=\"T_040a6_row5_col3\" class=\"data row5 col3\" >0.064379</td>\n",
       "      <td id=\"T_040a6_row5_col4\" class=\"data row5 col4\" >0.032253</td>\n",
       "      <td id=\"T_040a6_row5_col5\" class=\"data row5 col5\" >1.000000</td>\n",
       "      <td id=\"T_040a6_row5_col6\" class=\"data row5 col6\" >-0.030322</td>\n",
       "      <td id=\"T_040a6_row5_col7\" class=\"data row5 col7\" >0.212230</td>\n",
       "      <td id=\"T_040a6_row5_col8\" class=\"data row5 col8\" >0.023889</td>\n",
       "      <td id=\"T_040a6_row5_col9\" class=\"data row5 col9\" >-0.002844</td>\n",
       "      <td id=\"T_040a6_row5_col10\" class=\"data row5 col10\" >0.112804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_040a6_level0_row6\" class=\"row_heading level0 row6\" >Это прекрасная возможность для инвестиций</th>\n",
       "      <td id=\"T_040a6_row6_col0\" class=\"data row6 col0\" >0.035801</td>\n",
       "      <td id=\"T_040a6_row6_col1\" class=\"data row6 col1\" >-0.078566</td>\n",
       "      <td id=\"T_040a6_row6_col2\" class=\"data row6 col2\" >0.017175</td>\n",
       "      <td id=\"T_040a6_row6_col3\" class=\"data row6 col3\" >0.462397</td>\n",
       "      <td id=\"T_040a6_row6_col4\" class=\"data row6 col4\" >0.365505</td>\n",
       "      <td id=\"T_040a6_row6_col5\" class=\"data row6 col5\" >-0.030322</td>\n",
       "      <td id=\"T_040a6_row6_col6\" class=\"data row6 col6\" >1.000000</td>\n",
       "      <td id=\"T_040a6_row6_col7\" class=\"data row6 col7\" >0.282414</td>\n",
       "      <td id=\"T_040a6_row6_col8\" class=\"data row6 col8\" >0.267571</td>\n",
       "      <td id=\"T_040a6_row6_col9\" class=\"data row6 col9\" >0.285873</td>\n",
       "      <td id=\"T_040a6_row6_col10\" class=\"data row6 col10\" >-0.040309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_040a6_level0_row7\" class=\"row_heading level0 row7\" >C'est une merveilleuse opportunité d'investissement</th>\n",
       "      <td id=\"T_040a6_row7_col0\" class=\"data row7 col0\" >-0.062506</td>\n",
       "      <td id=\"T_040a6_row7_col1\" class=\"data row7 col1\" >-0.011418</td>\n",
       "      <td id=\"T_040a6_row7_col2\" class=\"data row7 col2\" >-0.006744</td>\n",
       "      <td id=\"T_040a6_row7_col3\" class=\"data row7 col3\" >0.092110</td>\n",
       "      <td id=\"T_040a6_row7_col4\" class=\"data row7 col4\" >0.566635</td>\n",
       "      <td id=\"T_040a6_row7_col5\" class=\"data row7 col5\" >0.212230</td>\n",
       "      <td id=\"T_040a6_row7_col6\" class=\"data row7 col6\" >0.282414</td>\n",
       "      <td id=\"T_040a6_row7_col7\" class=\"data row7 col7\" >1.000000</td>\n",
       "      <td id=\"T_040a6_row7_col8\" class=\"data row7 col8\" >0.292651</td>\n",
       "      <td id=\"T_040a6_row7_col9\" class=\"data row7 col9\" >0.187989</td>\n",
       "      <td id=\"T_040a6_row7_col10\" class=\"data row7 col10\" >0.006793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_040a6_level0_row8\" class=\"row_heading level0 row8\" >これは素晴らしい投資機会です</th>\n",
       "      <td id=\"T_040a6_row8_col0\" class=\"data row8 col0\" >0.027358</td>\n",
       "      <td id=\"T_040a6_row8_col1\" class=\"data row8 col1\" >0.090357</td>\n",
       "      <td id=\"T_040a6_row8_col2\" class=\"data row8 col2\" >0.010857</td>\n",
       "      <td id=\"T_040a6_row8_col3\" class=\"data row8 col3\" >0.314708</td>\n",
       "      <td id=\"T_040a6_row8_col4\" class=\"data row8 col4\" >0.172406</td>\n",
       "      <td id=\"T_040a6_row8_col5\" class=\"data row8 col5\" >0.023889</td>\n",
       "      <td id=\"T_040a6_row8_col6\" class=\"data row8 col6\" >0.267571</td>\n",
       "      <td id=\"T_040a6_row8_col7\" class=\"data row8 col7\" >0.292651</td>\n",
       "      <td id=\"T_040a6_row8_col8\" class=\"data row8 col8\" >1.000000</td>\n",
       "      <td id=\"T_040a6_row8_col9\" class=\"data row8 col9\" >0.577265</td>\n",
       "      <td id=\"T_040a6_row8_col10\" class=\"data row8 col10\" >-0.100630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_040a6_level0_row9\" class=\"row_heading level0 row9\" >野球はあなたが思うよりも面白いことがあります</th>\n",
       "      <td id=\"T_040a6_row9_col0\" class=\"data row9 col0\" >0.017622</td>\n",
       "      <td id=\"T_040a6_row9_col1\" class=\"data row9 col1\" >0.131507</td>\n",
       "      <td id=\"T_040a6_row9_col2\" class=\"data row9 col2\" >0.025510</td>\n",
       "      <td id=\"T_040a6_row9_col3\" class=\"data row9 col3\" >0.327675</td>\n",
       "      <td id=\"T_040a6_row9_col4\" class=\"data row9 col4\" >0.110118</td>\n",
       "      <td id=\"T_040a6_row9_col5\" class=\"data row9 col5\" >-0.002844</td>\n",
       "      <td id=\"T_040a6_row9_col6\" class=\"data row9 col6\" >0.285873</td>\n",
       "      <td id=\"T_040a6_row9_col7\" class=\"data row9 col7\" >0.187989</td>\n",
       "      <td id=\"T_040a6_row9_col8\" class=\"data row9 col8\" >0.577265</td>\n",
       "      <td id=\"T_040a6_row9_col9\" class=\"data row9 col9\" >1.000000</td>\n",
       "      <td id=\"T_040a6_row9_col10\" class=\"data row9 col10\" >-0.098722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_040a6_level0_row10\" class=\"row_heading level0 row10\" >Baseball can be interesting than you'd think</th>\n",
       "      <td id=\"T_040a6_row10_col0\" class=\"data row10 col0\" >0.023445</td>\n",
       "      <td id=\"T_040a6_row10_col1\" class=\"data row10 col1\" >0.016100</td>\n",
       "      <td id=\"T_040a6_row10_col2\" class=\"data row10 col2\" >0.006353</td>\n",
       "      <td id=\"T_040a6_row10_col3\" class=\"data row10 col3\" >-0.119607</td>\n",
       "      <td id=\"T_040a6_row10_col4\" class=\"data row10 col4\" >-0.013743</td>\n",
       "      <td id=\"T_040a6_row10_col5\" class=\"data row10 col5\" >0.112804</td>\n",
       "      <td id=\"T_040a6_row10_col6\" class=\"data row10 col6\" >-0.040309</td>\n",
       "      <td id=\"T_040a6_row10_col7\" class=\"data row10 col7\" >0.006793</td>\n",
       "      <td id=\"T_040a6_row10_col8\" class=\"data row10 col8\" >-0.100630</td>\n",
       "      <td id=\"T_040a6_row10_col9\" class=\"data row10 col9\" >-0.098722</td>\n",
       "      <td id=\"T_040a6_row10_col10\" class=\"data row10 col10\" >1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2df875360>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Compute similarities exactly the same as we did before!\n",
    "similarities = cosine_similarity(embeddings)\n",
    "\n",
    "# Turn into a dataframe\n",
    "pd.DataFrame(similarities,\n",
    "            index=sentences,\n",
    "            columns=sentences) \\\n",
    "            .style \\\n",
    "            .background_gradient(axis=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78bcf68",
   "metadata": {},
   "source": [
    "Sentence that are about the same thing don't match, just because they're in different languages! Don't worry, there are some [pretrained multilingual models](https://www.sbert.net/docs/pretrained_models.html#multi-lingual-models), let's see how much better that one looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "701e90fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v2')\n",
    "embeddings = model.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "507e4b1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_4db7a_row0_col0, #T_4db7a_row1_col1, #T_4db7a_row2_col2, #T_4db7a_row3_col3, #T_4db7a_row4_col4, #T_4db7a_row5_col5, #T_4db7a_row6_col6, #T_4db7a_row7_col7, #T_4db7a_row8_col8, #T_4db7a_row9_col9, #T_4db7a_row10_col10 {\n",
       "  background-color: #023858;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_4db7a_row0_col1, #T_4db7a_row1_col0 {\n",
       "  background-color: #93b5d6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_4db7a_row0_col2, #T_4db7a_row1_col2, #T_4db7a_row2_col0, #T_4db7a_row2_col1 {\n",
       "  background-color: #e6e2ef;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_4db7a_row0_col3, #T_4db7a_row3_col0, #T_4db7a_row7_col10, #T_4db7a_row10_col7 {\n",
       "  background-color: #d4d4e8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_4db7a_row0_col4, #T_4db7a_row1_col5, #T_4db7a_row1_col7, #T_4db7a_row4_col0, #T_4db7a_row5_col1, #T_4db7a_row7_col1 {\n",
       "  background-color: #f3edf5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_4db7a_row0_col5, #T_4db7a_row5_col0 {\n",
       "  background-color: #faf2f8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_4db7a_row0_col6, #T_4db7a_row0_col7, #T_4db7a_row6_col0, #T_4db7a_row7_col0 {\n",
       "  background-color: #faf3f9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_4db7a_row0_col8, #T_4db7a_row8_col0 {\n",
       "  background-color: #fff7fb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_4db7a_row0_col9, #T_4db7a_row1_col6, #T_4db7a_row6_col1, #T_4db7a_row9_col0 {\n",
       "  background-color: #f4edf6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_4db7a_row0_col10, #T_4db7a_row1_col4, #T_4db7a_row4_col1, #T_4db7a_row10_col0 {\n",
       "  background-color: #f0eaf4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_4db7a_row1_col3, #T_4db7a_row3_col1 {\n",
       "  background-color: #c9cee4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_4db7a_row1_col8, #T_4db7a_row8_col1 {\n",
       "  background-color: #fcf4fa;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_4db7a_row1_col9, #T_4db7a_row2_col8, #T_4db7a_row8_col2, #T_4db7a_row9_col1 {\n",
       "  background-color: #e9e5f1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_4db7a_row1_col10, #T_4db7a_row2_col7, #T_4db7a_row7_col2, #T_4db7a_row10_col1 {\n",
       "  background-color: #e4e1ef;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_4db7a_row2_col3, #T_4db7a_row3_col2 {\n",
       "  background-color: #7dacd1;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_4db7a_row2_col4, #T_4db7a_row4_col2 {\n",
       "  background-color: #2182b9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_4db7a_row2_col5, #T_4db7a_row5_col2 {\n",
       "  background-color: #e1dfed;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_4db7a_row2_col6, #T_4db7a_row3_col8, #T_4db7a_row6_col2, #T_4db7a_row8_col3, #T_4db7a_row8_col10, #T_4db7a_row10_col8 {\n",
       "  background-color: #dad9ea;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_4db7a_row2_col9, #T_4db7a_row3_col7, #T_4db7a_row7_col3, #T_4db7a_row9_col2 {\n",
       "  background-color: #d5d5e8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_4db7a_row2_col10, #T_4db7a_row6_col10, #T_4db7a_row10_col2, #T_4db7a_row10_col6 {\n",
       "  background-color: #d6d6e9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_4db7a_row3_col4, #T_4db7a_row4_col3 {\n",
       "  background-color: #96b6d7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_4db7a_row3_col5, #T_4db7a_row5_col3 {\n",
       "  background-color: #d9d8ea;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_4db7a_row3_col6, #T_4db7a_row6_col3 {\n",
       "  background-color: #cacee5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_4db7a_row3_col9, #T_4db7a_row9_col3 {\n",
       "  background-color: #e7e3f0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_4db7a_row3_col10, #T_4db7a_row10_col3 {\n",
       "  background-color: #e8e4f0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_4db7a_row4_col5, #T_4db7a_row4_col7, #T_4db7a_row4_col9, #T_4db7a_row5_col4, #T_4db7a_row7_col4, #T_4db7a_row9_col4 {\n",
       "  background-color: #c5cce3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_4db7a_row4_col6, #T_4db7a_row6_col4 {\n",
       "  background-color: #bbc7e0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_4db7a_row4_col8, #T_4db7a_row8_col4 {\n",
       "  background-color: #c8cde4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_4db7a_row4_col10, #T_4db7a_row10_col4 {\n",
       "  background-color: #d3d4e7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_4db7a_row5_col6, #T_4db7a_row6_col5 {\n",
       "  background-color: #034369;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_4db7a_row5_col7, #T_4db7a_row7_col5 {\n",
       "  background-color: #034165;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_4db7a_row5_col8, #T_4db7a_row6_col8, #T_4db7a_row8_col5, #T_4db7a_row8_col6 {\n",
       "  background-color: #03456c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_4db7a_row5_col9, #T_4db7a_row9_col5 {\n",
       "  background-color: #e5e1ef;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_4db7a_row5_col10, #T_4db7a_row10_col5 {\n",
       "  background-color: #d7d6e9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_4db7a_row6_col7, #T_4db7a_row7_col6 {\n",
       "  background-color: #023f64;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_4db7a_row6_col9, #T_4db7a_row7_col9, #T_4db7a_row9_col6, #T_4db7a_row9_col7 {\n",
       "  background-color: #e0deed;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_4db7a_row7_col8, #T_4db7a_row8_col7 {\n",
       "  background-color: #034267;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_4db7a_row8_col9, #T_4db7a_row9_col8 {\n",
       "  background-color: #e0dded;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_4db7a_row9_col10, #T_4db7a_row10_col9 {\n",
       "  background-color: #045d92;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_4db7a\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_4db7a_level0_col0\" class=\"col_heading level0 col0\" >Molly ate a fish</th>\n",
       "      <th id=\"T_4db7a_level0_col1\" class=\"col_heading level0 col1\" >Jen consumed a carp</th>\n",
       "      <th id=\"T_4db7a_level0_col2\" class=\"col_heading level0 col2\" >I would like to sell you a house</th>\n",
       "      <th id=\"T_4db7a_level0_col3\" class=\"col_heading level0 col3\" >Я пытаюсь купить дачу</th>\n",
       "      <th id=\"T_4db7a_level0_col4\" class=\"col_heading level0 col4\" >J'aimerais vous louer un grand appartement</th>\n",
       "      <th id=\"T_4db7a_level0_col5\" class=\"col_heading level0 col5\" >This is a wonderful investment opportunity</th>\n",
       "      <th id=\"T_4db7a_level0_col6\" class=\"col_heading level0 col6\" >Это прекрасная возможность для инвестиций</th>\n",
       "      <th id=\"T_4db7a_level0_col7\" class=\"col_heading level0 col7\" >C'est une merveilleuse opportunité d'investissement</th>\n",
       "      <th id=\"T_4db7a_level0_col8\" class=\"col_heading level0 col8\" >これは素晴らしい投資機会です</th>\n",
       "      <th id=\"T_4db7a_level0_col9\" class=\"col_heading level0 col9\" >野球はあなたが思うよりも面白いことがあります</th>\n",
       "      <th id=\"T_4db7a_level0_col10\" class=\"col_heading level0 col10\" >Baseball can be interesting than you'd think</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_4db7a_level0_row0\" class=\"row_heading level0 row0\" >Molly ate a fish</th>\n",
       "      <td id=\"T_4db7a_row0_col0\" class=\"data row0 col0\" >1.000000</td>\n",
       "      <td id=\"T_4db7a_row0_col1\" class=\"data row0 col1\" >0.358347</td>\n",
       "      <td id=\"T_4db7a_row0_col2\" class=\"data row0 col2\" >0.058340</td>\n",
       "      <td id=\"T_4db7a_row0_col3\" class=\"data row0 col3\" >0.145439</td>\n",
       "      <td id=\"T_4db7a_row0_col4\" class=\"data row0 col4\" >-0.024103</td>\n",
       "      <td id=\"T_4db7a_row0_col5\" class=\"data row0 col5\" >-0.070145</td>\n",
       "      <td id=\"T_4db7a_row0_col6\" class=\"data row0 col6\" >-0.075333</td>\n",
       "      <td id=\"T_4db7a_row0_col7\" class=\"data row0 col7\" >-0.073496</td>\n",
       "      <td id=\"T_4db7a_row0_col8\" class=\"data row0 col8\" >-0.111467</td>\n",
       "      <td id=\"T_4db7a_row0_col9\" class=\"data row0 col9\" >-0.025614</td>\n",
       "      <td id=\"T_4db7a_row0_col10\" class=\"data row0 col10\" >0.001549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4db7a_level0_row1\" class=\"row_heading level0 row1\" >Jen consumed a carp</th>\n",
       "      <td id=\"T_4db7a_row1_col0\" class=\"data row1 col0\" >0.358347</td>\n",
       "      <td id=\"T_4db7a_row1_col1\" class=\"data row1 col1\" >1.000000</td>\n",
       "      <td id=\"T_4db7a_row1_col2\" class=\"data row1 col2\" >0.059195</td>\n",
       "      <td id=\"T_4db7a_row1_col3\" class=\"data row1 col3\" >0.190241</td>\n",
       "      <td id=\"T_4db7a_row1_col4\" class=\"data row1 col4\" >-0.001941</td>\n",
       "      <td id=\"T_4db7a_row1_col5\" class=\"data row1 col5\" >-0.024359</td>\n",
       "      <td id=\"T_4db7a_row1_col6\" class=\"data row1 col6\" >-0.024816</td>\n",
       "      <td id=\"T_4db7a_row1_col7\" class=\"data row1 col7\" >-0.023295</td>\n",
       "      <td id=\"T_4db7a_row1_col8\" class=\"data row1 col8\" >-0.087019</td>\n",
       "      <td id=\"T_4db7a_row1_col9\" class=\"data row1 col9\" >0.040799</td>\n",
       "      <td id=\"T_4db7a_row1_col10\" class=\"data row1 col10\" >0.067243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4db7a_level0_row2\" class=\"row_heading level0 row2\" >I would like to sell you a house</th>\n",
       "      <td id=\"T_4db7a_row2_col0\" class=\"data row2 col0\" >0.058340</td>\n",
       "      <td id=\"T_4db7a_row2_col1\" class=\"data row2 col1\" >0.059195</td>\n",
       "      <td id=\"T_4db7a_row2_col2\" class=\"data row2 col2\" >1.000000</td>\n",
       "      <td id=\"T_4db7a_row2_col3\" class=\"data row2 col3\" >0.418692</td>\n",
       "      <td id=\"T_4db7a_row2_col4\" class=\"data row2 col4\" >0.642746</td>\n",
       "      <td id=\"T_4db7a_row2_col5\" class=\"data row2 col5\" >0.081795</td>\n",
       "      <td id=\"T_4db7a_row2_col6\" class=\"data row2 col6\" >0.118611</td>\n",
       "      <td id=\"T_4db7a_row2_col7\" class=\"data row2 col7\" >0.067805</td>\n",
       "      <td id=\"T_4db7a_row2_col8\" class=\"data row2 col8\" >0.042560</td>\n",
       "      <td id=\"T_4db7a_row2_col9\" class=\"data row2 col9\" >0.144491</td>\n",
       "      <td id=\"T_4db7a_row2_col10\" class=\"data row2 col10\" >0.139300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4db7a_level0_row3\" class=\"row_heading level0 row3\" >Я пытаюсь купить дачу</th>\n",
       "      <td id=\"T_4db7a_row3_col0\" class=\"data row3 col0\" >0.145439</td>\n",
       "      <td id=\"T_4db7a_row3_col1\" class=\"data row3 col1\" >0.190241</td>\n",
       "      <td id=\"T_4db7a_row3_col2\" class=\"data row3 col2\" >0.418692</td>\n",
       "      <td id=\"T_4db7a_row3_col3\" class=\"data row3 col3\" >1.000000</td>\n",
       "      <td id=\"T_4db7a_row3_col4\" class=\"data row3 col4\" >0.351605</td>\n",
       "      <td id=\"T_4db7a_row3_col5\" class=\"data row3 col5\" >0.120679</td>\n",
       "      <td id=\"T_4db7a_row3_col6\" class=\"data row3 col6\" >0.184644</td>\n",
       "      <td id=\"T_4db7a_row3_col7\" class=\"data row3 col7\" >0.144633</td>\n",
       "      <td id=\"T_4db7a_row3_col8\" class=\"data row3 col8\" >0.115598</td>\n",
       "      <td id=\"T_4db7a_row3_col9\" class=\"data row3 col9\" >0.050505</td>\n",
       "      <td id=\"T_4db7a_row3_col10\" class=\"data row3 col10\" >0.046084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4db7a_level0_row4\" class=\"row_heading level0 row4\" >J'aimerais vous louer un grand appartement</th>\n",
       "      <td id=\"T_4db7a_row4_col0\" class=\"data row4 col0\" >-0.024103</td>\n",
       "      <td id=\"T_4db7a_row4_col1\" class=\"data row4 col1\" >-0.001941</td>\n",
       "      <td id=\"T_4db7a_row4_col2\" class=\"data row4 col2\" >0.642746</td>\n",
       "      <td id=\"T_4db7a_row4_col3\" class=\"data row4 col3\" >0.351605</td>\n",
       "      <td id=\"T_4db7a_row4_col4\" class=\"data row4 col4\" >1.000000</td>\n",
       "      <td id=\"T_4db7a_row4_col5\" class=\"data row4 col5\" >0.203307</td>\n",
       "      <td id=\"T_4db7a_row4_col6\" class=\"data row4 col6\" >0.238716</td>\n",
       "      <td id=\"T_4db7a_row4_col7\" class=\"data row4 col7\" >0.204762</td>\n",
       "      <td id=\"T_4db7a_row4_col8\" class=\"data row4 col8\" >0.195163</td>\n",
       "      <td id=\"T_4db7a_row4_col9\" class=\"data row4 col9\" >0.201317</td>\n",
       "      <td id=\"T_4db7a_row4_col10\" class=\"data row4 col10\" >0.151998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4db7a_level0_row5\" class=\"row_heading level0 row5\" >This is a wonderful investment opportunity</th>\n",
       "      <td id=\"T_4db7a_row5_col0\" class=\"data row5 col0\" >-0.070145</td>\n",
       "      <td id=\"T_4db7a_row5_col1\" class=\"data row5 col1\" >-0.024359</td>\n",
       "      <td id=\"T_4db7a_row5_col2\" class=\"data row5 col2\" >0.081795</td>\n",
       "      <td id=\"T_4db7a_row5_col3\" class=\"data row5 col3\" >0.120679</td>\n",
       "      <td id=\"T_4db7a_row5_col4\" class=\"data row5 col4\" >0.203307</td>\n",
       "      <td id=\"T_4db7a_row5_col5\" class=\"data row5 col5\" >1.000000</td>\n",
       "      <td id=\"T_4db7a_row5_col6\" class=\"data row5 col6\" >0.953561</td>\n",
       "      <td id=\"T_4db7a_row5_col7\" class=\"data row5 col7\" >0.964282</td>\n",
       "      <td id=\"T_4db7a_row5_col8\" class=\"data row5 col8\" >0.945246</td>\n",
       "      <td id=\"T_4db7a_row5_col9\" class=\"data row5 col9\" >0.062618</td>\n",
       "      <td id=\"T_4db7a_row5_col10\" class=\"data row5 col10\" >0.133220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4db7a_level0_row6\" class=\"row_heading level0 row6\" >Это прекрасная возможность для инвестиций</th>\n",
       "      <td id=\"T_4db7a_row6_col0\" class=\"data row6 col0\" >-0.075333</td>\n",
       "      <td id=\"T_4db7a_row6_col1\" class=\"data row6 col1\" >-0.024816</td>\n",
       "      <td id=\"T_4db7a_row6_col2\" class=\"data row6 col2\" >0.118611</td>\n",
       "      <td id=\"T_4db7a_row6_col3\" class=\"data row6 col3\" >0.184644</td>\n",
       "      <td id=\"T_4db7a_row6_col4\" class=\"data row6 col4\" >0.238716</td>\n",
       "      <td id=\"T_4db7a_row6_col5\" class=\"data row6 col5\" >0.953561</td>\n",
       "      <td id=\"T_4db7a_row6_col6\" class=\"data row6 col6\" >1.000000</td>\n",
       "      <td id=\"T_4db7a_row6_col7\" class=\"data row6 col7\" >0.968368</td>\n",
       "      <td id=\"T_4db7a_row6_col8\" class=\"data row6 col8\" >0.944719</td>\n",
       "      <td id=\"T_4db7a_row6_col9\" class=\"data row6 col9\" >0.084221</td>\n",
       "      <td id=\"T_4db7a_row6_col10\" class=\"data row6 col10\" >0.136699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4db7a_level0_row7\" class=\"row_heading level0 row7\" >C'est une merveilleuse opportunité d'investissement</th>\n",
       "      <td id=\"T_4db7a_row7_col0\" class=\"data row7 col0\" >-0.073496</td>\n",
       "      <td id=\"T_4db7a_row7_col1\" class=\"data row7 col1\" >-0.023295</td>\n",
       "      <td id=\"T_4db7a_row7_col2\" class=\"data row7 col2\" >0.067805</td>\n",
       "      <td id=\"T_4db7a_row7_col3\" class=\"data row7 col3\" >0.144633</td>\n",
       "      <td id=\"T_4db7a_row7_col4\" class=\"data row7 col4\" >0.204762</td>\n",
       "      <td id=\"T_4db7a_row7_col5\" class=\"data row7 col5\" >0.964282</td>\n",
       "      <td id=\"T_4db7a_row7_col6\" class=\"data row7 col6\" >0.968368</td>\n",
       "      <td id=\"T_4db7a_row7_col7\" class=\"data row7 col7\" >1.000000</td>\n",
       "      <td id=\"T_4db7a_row7_col8\" class=\"data row7 col8\" >0.959357</td>\n",
       "      <td id=\"T_4db7a_row7_col9\" class=\"data row7 col9\" >0.086458</td>\n",
       "      <td id=\"T_4db7a_row7_col10\" class=\"data row7 col10\" >0.146568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4db7a_level0_row8\" class=\"row_heading level0 row8\" >これは素晴らしい投資機会です</th>\n",
       "      <td id=\"T_4db7a_row8_col0\" class=\"data row8 col0\" >-0.111467</td>\n",
       "      <td id=\"T_4db7a_row8_col1\" class=\"data row8 col1\" >-0.087019</td>\n",
       "      <td id=\"T_4db7a_row8_col2\" class=\"data row8 col2\" >0.042560</td>\n",
       "      <td id=\"T_4db7a_row8_col3\" class=\"data row8 col3\" >0.115598</td>\n",
       "      <td id=\"T_4db7a_row8_col4\" class=\"data row8 col4\" >0.195163</td>\n",
       "      <td id=\"T_4db7a_row8_col5\" class=\"data row8 col5\" >0.945246</td>\n",
       "      <td id=\"T_4db7a_row8_col6\" class=\"data row8 col6\" >0.944719</td>\n",
       "      <td id=\"T_4db7a_row8_col7\" class=\"data row8 col7\" >0.959357</td>\n",
       "      <td id=\"T_4db7a_row8_col8\" class=\"data row8 col8\" >1.000000</td>\n",
       "      <td id=\"T_4db7a_row8_col9\" class=\"data row8 col9\" >0.091451</td>\n",
       "      <td id=\"T_4db7a_row8_col10\" class=\"data row8 col10\" >0.115392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4db7a_level0_row9\" class=\"row_heading level0 row9\" >野球はあなたが思うよりも面白いことがあります</th>\n",
       "      <td id=\"T_4db7a_row9_col0\" class=\"data row9 col0\" >-0.025614</td>\n",
       "      <td id=\"T_4db7a_row9_col1\" class=\"data row9 col1\" >0.040799</td>\n",
       "      <td id=\"T_4db7a_row9_col2\" class=\"data row9 col2\" >0.144491</td>\n",
       "      <td id=\"T_4db7a_row9_col3\" class=\"data row9 col3\" >0.050505</td>\n",
       "      <td id=\"T_4db7a_row9_col4\" class=\"data row9 col4\" >0.201317</td>\n",
       "      <td id=\"T_4db7a_row9_col5\" class=\"data row9 col5\" >0.062618</td>\n",
       "      <td id=\"T_4db7a_row9_col6\" class=\"data row9 col6\" >0.084221</td>\n",
       "      <td id=\"T_4db7a_row9_col7\" class=\"data row9 col7\" >0.086458</td>\n",
       "      <td id=\"T_4db7a_row9_col8\" class=\"data row9 col8\" >0.091451</td>\n",
       "      <td id=\"T_4db7a_row9_col9\" class=\"data row9 col9\" >1.000000</td>\n",
       "      <td id=\"T_4db7a_row9_col10\" class=\"data row9 col10\" >0.839617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4db7a_level0_row10\" class=\"row_heading level0 row10\" >Baseball can be interesting than you'd think</th>\n",
       "      <td id=\"T_4db7a_row10_col0\" class=\"data row10 col0\" >0.001549</td>\n",
       "      <td id=\"T_4db7a_row10_col1\" class=\"data row10 col1\" >0.067243</td>\n",
       "      <td id=\"T_4db7a_row10_col2\" class=\"data row10 col2\" >0.139300</td>\n",
       "      <td id=\"T_4db7a_row10_col3\" class=\"data row10 col3\" >0.046084</td>\n",
       "      <td id=\"T_4db7a_row10_col4\" class=\"data row10 col4\" >0.151998</td>\n",
       "      <td id=\"T_4db7a_row10_col5\" class=\"data row10 col5\" >0.133220</td>\n",
       "      <td id=\"T_4db7a_row10_col6\" class=\"data row10 col6\" >0.136699</td>\n",
       "      <td id=\"T_4db7a_row10_col7\" class=\"data row10 col7\" >0.146568</td>\n",
       "      <td id=\"T_4db7a_row10_col8\" class=\"data row10 col8\" >0.115392</td>\n",
       "      <td id=\"T_4db7a_row10_col9\" class=\"data row10 col9\" >0.839617</td>\n",
       "      <td id=\"T_4db7a_row10_col10\" class=\"data row10 col10\" >1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2bbcbfa00>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Compute similarities exactly the same as we did before!\n",
    "similarities = cosine_similarity(embeddings)\n",
    "\n",
    "# Turn into a dataframe\n",
    "pd.DataFrame(similarities,\n",
    "            index=sentences,\n",
    "            columns=sentences) \\\n",
    "            .style \\\n",
    "            .background_gradient(axis=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314a0300",
   "metadata": {},
   "source": [
    "If you're interested in how this might work with GPT or other large language model, you might want to check out [this GitHub repo](https://github.com/jsoma/mediaparty-folktales)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0063df5e",
   "metadata": {},
   "source": [
    "## Limitations of large language models\n",
    "\n",
    "The **knowledge cutoff** is a big one, where they only know what's in their training data. In the case of GPT, it stopped getting new information in September 2021.\n",
    "\n",
    "The **token limit** is another issue, where you can only provide so much information to the AI tool at a time, usually several thousand words. Newer language models have larger token limits – there is a version of GPT-4 that allows 32k tokens, which is around 24,000 words – and AnthropicAI's Claude [has a 100k context window](https://www.anthropic.com/index/100k-context-windows). This is definitely one area where providers are competing!\n",
    "\n",
    "There are ways to get around these problems, which you can see some details of [in my Media Party Chicago 2023 talk](https://github.com/jsoma/mediaparty-folktales). Technically speaking it's called \"in-context learning with semantic search,\" and it's the baseline of all of the \"chat with your PDF\" kind of tools you might have seen.\n",
    "\n",
    "The biggest limitation of an LLM is **hallucinations**, where it will just make things up! My two suggestions for this are **don't use LLMs as a knowledge base** and **always fact-check**. Even simple tasks are prone to hallucinations: [a Danish newspaper found GPT-generated article summaries often contained information not in the original piece](https://generative-ai-newsroom.com/summaries-in-danish-with-openai-cbb814a119f2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8630eda",
   "metadata": {},
   "source": [
    "## Image-based models\n",
    "\n",
    "Image-based models can allow you to do things like research using aerial and satellite photography, as was done in [Leprosy of the Land](https://texty.org.ua/d/2018/amber_eng/) when computer vision could detect illegal amber mines.\n",
    "\n",
    "You can view code samples and get more ideas by browsing [the Hugging Face website](https://huggingface.co/models?sort=trending).\n",
    "\n",
    "### Image classification\n",
    "\n",
    "[Leprosy of the Land](https://texty.org.ua/d/2018/amber_eng/) was an **image classification** problem, which is practically the same as text classification! If you wanted to do a similar problem, you can similarly train a custom model or fine-tune an existing image model.\n",
    "\n",
    "To fine-tune an existing image model, I again recommend [Hugging Face AutoTrain](https://ui.autotrain.huggingface.co/). You upload folders with several examples of each category or class you're looking for, and it automatically creates a model for you.\n",
    "\n",
    "You can see [a tutorial I gave on this (with data) here on GitHub](http://github.com/jsoma/nicar23-huggingface), or [the completed model here](https://huggingface.co/wendys-llc/autotrain-amber-mines-42327108538).\n",
    "\n",
    "<iframe\n",
    "\tsrc=\"https://datatrooper-zero-shot-image-classification.hf.space\"\n",
    "\tframeborder=\"0\"\n",
    "\twidth=\"850\"\n",
    "\theight=\"1050\"\n",
    "></iframe>\n",
    "\n",
    "### Semantic segmentation\n",
    "\n",
    "Another great tool to use with images is **semantic segmentation**. This allows you to classify different pixels in an image.\n",
    "\n",
    "This is common for using aerial or satellite photography to determine land use, finding asphalt/roads, or greenery or crops. You can learn more about that [at Normal AI](https://normalai.org/images/semantic-segmentation.html).\n",
    "\n",
    "<iframe\n",
    "\tsrc=\"https://wendys-llc-panoptic-segment-anything.hf.space\"\n",
    "\tframeborder=\"0\"\n",
    "\twidth=\"850\"\n",
    "\theight=\"650\"\n",
    "></iframe>\n",
    "\n",
    "### Object detection\n",
    "\n",
    "Object detect might be the easiest to explain: it detects objects in images! It doesn't always do a perfect job, though, as sometimes you want specific objects that the pre-trained models don't know about.\n",
    "\n",
    "You can learn more about that [at Normal AI](https://normalai.org/images/instance-segmentation.html).\n",
    "\n",
    "<iframe\n",
    "\tsrc=\"https://adirik-owl-vit.hf.space\"\n",
    "\tframeborder=\"0\"\n",
    "\twidth=\"850\"\n",
    "\theight=\"1450\"\n",
    "></iframe>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfa2e2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
